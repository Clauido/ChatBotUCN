{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliente de chroma por http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para levantar chroma como servidor en la máquina(Hay que instalar chroma) -> `pip install chroma`\n",
    "> Ejecutar en terminal `chroma run --path /db_path`\n",
    "- /db_path debe de ser el path donde está la base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obetener la colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglamentos = client.get_collection(name='reglamentos_UCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0aa3f4a5-f8b3-41af-b096-c8e3e294e2c6',\n",
       "  '51ca50d7-0ff5-439b-9f36-b22b9cce638a',\n",
       "  '1d317e0d-79e9-4849-b0c8-7818952afaa7',\n",
       "  '4507fecb-67a5-4a1e-88f2-d2ea4eeec15a',\n",
       "  'b0c586f2-1450-45fe-bec1-7988b0726c17',\n",
       "  'baa12f8b-0008-493d-b4f6-36b9ccda9162',\n",
       "  '2cce0edc-ff32-4c9e-8c1b-12646b01d5c6',\n",
       "  '71d72293-fe42-450b-9860-81670f3206c2',\n",
       "  'd74c0db2-6ec8-4114-9ef8-7328377153f4',\n",
       "  '1e342d36-a7b5-4614-90d8-cc431a87f2e9'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'page': 0, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 1, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 2, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 2, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 3, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 4, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 4, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 5, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 6, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'},\n",
       "  {'page': 7, 'source': '..\\\\PDF\\\\HAI_2024_AI-Index-Report.pdf'}],\n",
       " 'documents': ['Artificial Intelligence Index Report 2024',\n",
       "  'Artificial Intelligence Index Report 20242Introduction to the AI Index Report 2024 Welcome to the seventh edition of the AI Index report. The 2024 Index is our most comprehensive to date and arrives at an important moment when AI’s influence on society has never been more pronounced. This year, we have broadened our scope to more extensively cover essential trends such as technical advancements in AI, public perceptions of the technology, and the geopolitical dynamics surrounding its development. Featuring more original data than ever before, this edition introduces new estimates on AI training costs, detailed analyses of the responsible AI landscape, and an entirely new chapter dedicated to AI’s impact on science and medicine. The AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI). Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions have been cited in major newspapers, including the The New York Times, Bloomberg, and The Guardian, have amassed hundreds of academic citations, and been referenced by high-level policymakers in the United States, the United Kingdom, and the European Union, among other places. This year’s edition surpasses all previous ones in size, scale, and scope, reflecting the growing significance that AI is coming to hold in all of our lives.',\n",
       "  'Artificial Intelligence Index Report 20243Message From the Co-directors A decade ago, the best AI systems in the world were unable to classify objects in images at a human level. AI struggled with language comprehension and could not solve math problems. Today, AI systems routinely exceed human performance on standard benchmarks. Progress accelerated in 2023. New state-of-the-art systems like GPT-4, Gemini, and Claude 3 are impressively multimodal: They can generate fluent text in dozens of languages, process audio, and even explain memes. As AI has improved, it has increasingly forced its way into our lives. Companies are racing to build AI-based products, and AI is increasingly being used by the general public. But current AI technology still has significant problems. It cannot reliably deal with facts, perform complex reasoning, or explain its conclusions. AI faces two interrelated futures. First, technology continues to improve and is increasingly used, having major consequences for productivity and employment. It can be put to both good and bad uses. In the second future, the adoption of AI is constrained by the limitations of the technology. Regardless of which future unfolds, governments are increasingly concerned. They are stepping in to encourage the upside, such as funding university R&D and incentivizing private investment. Governments are also aiming to manage the potential downsides, such as impacts on employment, privacy concerns, misinformation, and intellectual property rights. As AI rapidly evolves, the AI Index aims to help the AI community, policymakers, business leaders, journalists, and the general public navigate this complex landscape. It provides ongoing, objective snapshots tracking several key areas: technical progress in AI capabilities, the community and investments driving AI development and deployment, public opinion on current and potential future impacts, and policy measures taken to stimulate AI innovation while managing its risks',\n",
       "  'property rights. As AI rapidly evolves, the AI Index aims to help the AI community, policymakers, business leaders, journalists, and the general public navigate this complex landscape. It provides ongoing, objective snapshots tracking several key areas: technical progress in AI capabilities, the community and investments driving AI development and deployment, public opinion on current and potential future impacts, and policy measures taken to stimulate AI innovation while managing its risks and challenges. By comprehensively monitoring the AI ecosystem, the Index serves as an important resource for understanding this transformative technological force. On the technical front, this year’s AI Index reports that the number of new large language models released worldwide in 2023 doubled over the previous year. Two-thirds were open-source, but the highest-performing models came from industry players with closed systems. Gemini Ultra became the first LLM to reach human- level performance on the Massive Multitask Language Understanding (MMLU) benchmark; performance on the benchmark has improved by 15 percentage points since last year. Additionally, GPT-4 achieved an impressive 0.96 mean win rate score on the comprehensive Holistic Evaluation of Language Models (HELM) benchmark, which includes MMLU among other evaluations.',\n",
       "  'Artificial Intelligence Index Report 20244Although global private investment in AI decreased for the second consecutive year, investment in generative AI skyrocketed. More Fortune 500 earnings calls mentioned AI than ever before, and new studies show that AI tangibly boosts worker productivity. On the policymaking front, global mentions of AI in legislative proceedings have never been higher. U.S. regulators passed more AI-related regulations in 2023 than ever before. Still, many expressed concerns about AI’s ability to generate deepfakes and impact elections. The public became more aware of AI, and studies suggest that they responded with nervousness. Ray Perrault and Jack Clark Co-directors, AI IndexMessage From the Co-directors (cont’d)',\n",
       "  'Artificial Intelligence Index Report 20245Top 10 Takeaways 1. AI beats humans on some tasks, but not on all. AI has surpassed human performance on several benchmarks, including some in image classification, visual reasoning, and English understanding. Yet it trails behind on more complex tasks like competition-level mathematics, visual commonsense reasoning and planning. 2. Industry continues to dominate frontier AI research. In 2023, industry produced 51 notable machine learning models, while academia contributed only 15. There were also 21 notable models resulting from industry-academia collaborations in 2023, a new high. 3. Frontier models get way more expensive. According to AI Index estimates, the training costs of state-of-the-art AI models have reached unprecedented levels. For example, OpenAI’s GPT-4 used an estimated $78 million worth of compute to train, while Google’s Gemini Ultra cost $191 million for compute. 4. The United States leads China, the EU, and the U.K. as the leading source of top AI models. In 2023, 61 notable AI models originated from U.S.-based institutions, far outpacing the European Union’s 21 and China’s 15. 5. Robust and standardized evaluations for LLM responsibility are seriously lacking. New research from the AI Index reveals a significant lack of standardization in responsible AI reporting. Leading developers, including OpenAI, Google, and Anthropic, primarily test their models against different responsible AI benchmarks. This practice complicates efforts to systematically compare the risks and limitations of top AI models. 6. Generative AI investment skyrockets. Despite a decline in overall AI private investment last year, funding for generative AI surged, nearly octupling from 2022 to reach $25.2 billion. Major players in the generative AI space, including OpenAI, Anthropic, Hugging Face, and Inflection, reported substantial fundraising rounds. 7 . The data is in: AI makes workers more productive and leads to higher quality work.',\n",
       "  'efforts to systematically compare the risks and limitations of top AI models. 6. Generative AI investment skyrockets. Despite a decline in overall AI private investment last year, funding for generative AI surged, nearly octupling from 2022 to reach $25.2 billion. Major players in the generative AI space, including OpenAI, Anthropic, Hugging Face, and Inflection, reported substantial fundraising rounds. 7 . The data is in: AI makes workers more productive and leads to higher quality work. In 2023, several studies assessed AI’s impact on labor, suggesting that AI enables workers to complete tasks more quickly and to improve the quality of their output. These studies also demonstrated AI’s potential to bridge the skill gap between low- and high-skilled workers. Still, other studies caution that using AI without proper oversight can lead to diminished performance. 5',\n",
       "  'Artificial Intelligence Index Report 20246Top 10 Takeaways (cont’d) 8. Scientific progress accelerates even further, thanks to AI. In 2022, AI began to advance scientific discovery. 2023, however, saw the launch of even more significant science-related AI applications— from AlphaDev, which makes algorithmic sorting more efficient, to GNoME, which facilitates the process of materials discovery. 9. The number of AI regulations in the United States sharply increases. The number of AI- related regulations in the U.S. has risen significantly in the past year and over the last five years. In 2023, there were 25 AI-related regulations, up from just one in 2016. Last year alone, the total number of AI-related regulations grew by 56.3%. 10. People across the globe are more cognizant of AI’s potential impact—and more nervous. A survey from Ipsos shows that, over the last year, the proportion of those who think AI will dramatically affect their lives in the next three to five years has increased from 60% to 66%. Moreover, 52% express nervousness toward AI products and services, marking a 13 percentage point rise from 2022. In America, Pew data suggests that 52% of Americans report feeling more concerned than excited about AI, rising from 37% in 2022.',\n",
       "  'Artificial Intelligence Index Report 20247Steering Committee Staff and ResearchersCo-directors MembersJack Clark, Anthropic, OECD Raymond Perrault, SRI International Erik Brynjolfsson, Stanford University John Etchemendy, Stanford University Katrina Ligett, Hebrew University Terah Lyons, JPMorgan Chase & Co. James Manyika, Google, University of OxfordJuan Carlos Niebles, Stanford University, Salesforce Vanessa Parli, Stanford University Yoav Shoham, Stanford University, AI21 Labs Russell Wald, Stanford University Research Manager and Editor in Chief Research Associate Affiliated Researchers Graduate ResearchersNestor Maslej Stanford University Loredana Fattorini Stanford University James da Costa, Stanford University Simba Jonga, Stanford UniversityElif Kiesow Cortez, Stanford Law School Research Fellow Anka Reuel, Stanford University Robi Rahman, Data ScientistAlexandra Rome, Freelance Researcher Lapo Santarlasci, IMT School for Advanced Studies Lucca Undergraduate Researchers Emily Capstick, Stanford University Summer Flowers, Stanford University Armin Hamrah, Claremont McKenna College Amelia Hardy, Stanford University Mena Hassan, Stanford University Ethan Duncan He-Li Hellman, Stanford UniversityJulia Betts Lotufo, Stanford University Sukrut Oak, Stanford University Andrew Shi, Stanford University Jason Shin, Stanford University Emma Williamson, Stanford University Alfred Yu, Stanford University',\n",
       "  'Artificial Intelligence Index Report 20248How to Cite This Report Public Data and Tools AI Index and Stanford HAINestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, and Jack Clark,  \\n“The AI Index 2024 Annual Report,” AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2024. The AI Index 2024 Annual Report by Stanford University is licensed under Attribution-NoDerivatives 4.0 International. The AI Index 2024 Report is supplemented by raw data and an interactive tool. We invite each reader to use the data and the tool in a way most relevant to their work and interests.\\n • Raw data and charts: The public data and high-resolution images of all the charts in the report are available on Google Drive.\\n • Global AI Vibrancy Tool: Compare the AI ecosystems of over 30 countries. The Global AI Vibrancy tool will be updated in the summer of 2024. The AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). The AI Index was conceived within the One Hundred Year Study on Artificial Intelligence (AI100). The AI Index welcomes feedback and new ideas for next year. Contact us at AI-Index-Report@stanford.edu. The AI Index acknowledges that while authored by a team of human researchers, its writing process was aided by AI tools. Specifically, the authors used ChatGPT and Claude to help tighten and copy edit initial drafts. The workflow involved authors writing the original copy, then utilizing AI tools as part of the editing process.'],\n",
       " 'data': None,\n",
       " 'uris': None,\n",
       " 'included': ['documents', 'metadatas']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reglamentos.count()\n",
    "reglamentos.get(limit=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otener los 10 primeros chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3a7bebf1-be2c-44b5-87bc-017d900747ef',\n",
       "   '6014ea2a-a8d7-4f5a-801c-a03cf02e3dae',\n",
       "   'cb8a25d9-2b8a-41cd-8c18-2e51a41dc627',\n",
       "   'd5669e9b-a24c-44df-be60-609a775fb865',\n",
       "   '78f9e8e8-97b8-4ec0-a213-382c2671cee9',\n",
       "   'd28e3894-aa30-4aa4-a4d0-2fe4c68bf9cd',\n",
       "   'eca57408-3b2b-4533-97c0-1b0d038d4617',\n",
       "   '3a3d5bc6-3f44-4ad5-b88c-506eb70fd46a',\n",
       "   '498cc06b-d579-452c-8a2a-c6efd3e63279',\n",
       "   'f0b57e70-3a20-4d52-8118-5bbb177de741']],\n",
       " 'distances': [[1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8845523595809937,\n",
       "   1.8845523595809937]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 150,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 150,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''}]],\n",
       " 'documents': [['Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   '151\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2 Preview\\nTable of Contents\\n2.12 Techniques for LLM Improvement\\nChapter 2: Technical Performance\\nArtificial Intelligence\\nIndex Report 2024\\nFine-Tuning\\nFine-tuning has grown increasingly popular as a \\nmethod of enhancing LLMs and involves further \\ntraining or adjusting models on smaller datasets.  \\nFine-tuning not only boosts overall model \\nperformance but also sharpens the model’s \\ncapabilities on specific tasks. It also allows for more \\nprecise control over the model’s behavior.\\n879\\n902\\n916\\n966\\n974\\n992\\n1,022\\n1,348\\nGuanaco 7B\\nBard\\nGuanaco 13B\\nChatGPT\\nVicuna 13B\\nGuanaco 33B\\nGuanaco 65B\\nGPT-4\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\nElo rating (mean)\\nModel competitions based on 10,000 simulations using GPT-4 and the Vicuna benchmark\\nSource: Dettmers et al., 2023 | Chart: 2024 AI Index report\\nFigure 2.12.5\\nQLoRA\\nHighlighted Research:\\nQLoRA, developed by researchers from the \\nUniversity of Washington in 2023, is a new method \\nfor more efficient model fine-tuning. It dramatically \\nreduces memory usage, enabling the fine-tuning \\nof a 65 billion parameter model on a single 48 \\nGB GPU while maintaining full 16-bit fine-tuning \\nperformance. To put this in perspective, fine-tuning \\na 65B Llama model, a leading open-source LLM, \\ntypically requires about 780 GB of GPU memory. \\nTherefore, QLoRA is nearly 16 times more efficient.\\nQLoRA manages to increase efficiency with \\ntechniques like a 4-bit NormalFloat (NF4), double \\nquantization, and page optimizers. QLoRA is \\nused to train a model named Guanaco, which \\nmatched or even surpassed models like ChatGPT \\nin performance on the Vicuna benchmark (a \\nbenchmark that ranks the outputs of LLMs) (Figure \\n2.12.5). Remarkably, the Guanaco models were \\ncreated with just 24 hours of fine-tuning on a single \\nGPU. QLoRa highlights how methods for optimizing \\nand further improving models have become more \\nefficient, meaning fewer resources will be required \\nto make increasingly capable models.',\n",
       "   '151\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2 Preview\\nTable of Contents\\n2.12 Techniques for LLM Improvement\\nChapter 2: Technical Performance\\nArtificial Intelligence\\nIndex Report 2024\\nFine-Tuning\\nFine-tuning has grown increasingly popular as a \\nmethod of enhancing LLMs and involves further \\ntraining or adjusting models on smaller datasets.  \\nFine-tuning not only boosts overall model \\nperformance but also sharpens the model’s \\ncapabilities on specific tasks. It also allows for more \\nprecise control over the model’s behavior.\\n879\\n902\\n916\\n966\\n974\\n992\\n1,022\\n1,348\\nGuanaco 7B\\nBard\\nGuanaco 13B\\nChatGPT\\nVicuna 13B\\nGuanaco 33B\\nGuanaco 65B\\nGPT-4\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\nElo rating (mean)\\nModel competitions based on 10,000 simulations using GPT-4 and the Vicuna benchmark\\nSource: Dettmers et al., 2023 | Chart: 2024 AI Index report\\nFigure 2.12.5\\nQLoRA\\nHighlighted Research:\\nQLoRA, developed by researchers from the \\nUniversity of Washington in 2023, is a new method \\nfor more efficient model fine-tuning. It dramatically \\nreduces memory usage, enabling the fine-tuning \\nof a 65 billion parameter model on a single 48 \\nGB GPU while maintaining full 16-bit fine-tuning \\nperformance. To put this in perspective, fine-tuning \\na 65B Llama model, a leading open-source LLM, \\ntypically requires about 780 GB of GPU memory. \\nTherefore, QLoRA is nearly 16 times more efficient.\\nQLoRA manages to increase efficiency with \\ntechniques like a 4-bit NormalFloat (NF4), double \\nquantization, and page optimizers. QLoRA is \\nused to train a model named Guanaco, which \\nmatched or even surpassed models like ChatGPT \\nin performance on the Vicuna benchmark (a \\nbenchmark that ranks the outputs of LLMs) (Figure \\n2.12.5). Remarkably, the Guanaco models were \\ncreated with just 24 hours of fine-tuning on a single \\nGPU. QLoRa highlights how methods for optimizing \\nand further improving models have become more \\nefficient, meaning fewer resources will be required \\nto make increasingly capable models.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['distances', 'documents', 'metadatas']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_report_collection.query(\n",
    "    query_texts=[\"QLoRA\"],\n",
    "    where_document={\"$contains\":\"QLoRA\"},\n",
    "    n_results=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thisEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
