{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliente de chroma por http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para levantar chroma como servidor en la máquina(Hay que instalar chroma) -> `pip install chroma`\n",
    "> Ejecutar en terminal `chroma run --path /db_path`\n",
    "- /db_path debe de ser el path donde está la base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obetener la colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name='stanford_report_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(id=aaf5f4ac-f71b-41d4-bb2b-e59a686f35b6, name=stanford_report_data)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections= client.list_collections()\n",
    "stanford_report_collection=collections[0]\n",
    "stanford_report_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otener los 10 primeros chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_report_collection.peek(limit= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3a7bebf1-be2c-44b5-87bc-017d900747ef',\n",
       "   '6014ea2a-a8d7-4f5a-801c-a03cf02e3dae',\n",
       "   'cb8a25d9-2b8a-41cd-8c18-2e51a41dc627',\n",
       "   'd5669e9b-a24c-44df-be60-609a775fb865',\n",
       "   '78f9e8e8-97b8-4ec0-a213-382c2671cee9',\n",
       "   'd28e3894-aa30-4aa4-a4d0-2fe4c68bf9cd',\n",
       "   'eca57408-3b2b-4533-97c0-1b0d038d4617',\n",
       "   '3a3d5bc6-3f44-4ad5-b88c-506eb70fd46a',\n",
       "   '498cc06b-d579-452c-8a2a-c6efd3e63279',\n",
       "   'f0b57e70-3a20-4d52-8118-5bbb177de741']],\n",
       " 'distances': [[1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8000057935714722,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8358268737792969,\n",
       "   1.8845523595809937,\n",
       "   1.8845523595809937]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 74,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 467,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 150,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''},\n",
       "   {'author': '',\n",
       "    'creationDate': \"D:20240423092512-07'00'\",\n",
       "    'creator': 'Adobe InDesign 19.3 (Macintosh)',\n",
       "    'file_path': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'format': 'PDF 1.7',\n",
       "    'keywords': '',\n",
       "    'modDate': \"D:20240423093238-07'00'\",\n",
       "    'page': 150,\n",
       "    'producer': 'Adobe PDF Library 17.0',\n",
       "    'source': '../PDF/HAI_2024_AI-Index-Report.pdf',\n",
       "    'subject': '',\n",
       "    'title': '',\n",
       "    'total_pages': 502,\n",
       "    'trapped': ''}]],\n",
       " 'documents': [['Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Moral Reasoning\\t\\n122\\n\\t\\nMoCa\\t\\n\\t\\n122\\nCausal Reasoning\\t\\n124\\n\\t\\nBigToM\\t\\n\\t\\n124\\n\\t\\nHighlighted Research:  \\n\\t\\nTübingen Cause-Effect Pairs\\t\\n126\\n2.7 Audio\\t \\t\\n127\\nGeneration\\t \\t\\n127\\n\\t\\nHighlighted Research: UniAudio\\t\\n128\\n\\t\\nHighlighted Research:  \\n\\t\\nMusicGEN and MusicLM\\t\\n129\\n2.8 Agents\\t\\t\\n131\\nGeneral Agents\\t\\n131\\n\\t\\nAgentBench\\t\\n131\\n\\t\\nHighlighted Research: Voyageur\\t\\n133\\nTask-Specific Agents\\t\\n134\\n\\t\\nMLAgentBench\\t\\n134\\n2.9 Robotics\\t\\n135\\n\\t\\nHighlighted Research: PaLM-E\\t\\n135\\n\\t\\nHighlighted Research: RT-2\\t\\n137\\n2.10 Reinforcement Learning\\t\\n138\\nReinforcement Learning from Human Feedback\\t\\n138\\n\\t\\nHighlighted Research: RLAIF\\t\\n139\\n\\t\\nHighlighted Research:  \\n\\t\\nDirect Preference Optimization\\t\\n140\\n2.11 Properties of LLMs\\t\\n141\\n\\t\\nHighlighted Research:  \\n\\t\\nChallenging the Notion of Emergent Behavior\\t 141\\n\\t\\nHighlighted Research:  \\n\\t\\nChanges in LLM Performance Over Time\\t\\n143\\n\\t\\nHighlighted Research:  \\n\\t\\nLLMs Are Poor Self-Correctors\\t\\n145\\n\\t\\nClosed vs. Open Model Performance\\t\\n146\\n2.12 Techniques for LLM Improvement\\t\\n148\\nPrompting\\t\\n\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nGraph of Thoughts Prompting\\t\\n148\\n\\t\\nHighlighted Research:  \\n\\t\\nOptimization by PROmpting (OPRO)\\t\\n150\\nFine-Tuning\\t \\t\\n151\\n\\t\\nHighlighted Research: QLoRA\\t\\n151\\nAttention\\t\\n\\t\\n152\\n\\t\\nHighlighted Research: Flash-Decoding\\t\\n152\\n2.13 Environmental Impact of AI Systems\\t154\\nGeneral Environmental Impact\\t\\n154\\n\\t\\nTraining\\t \\t\\n154\\n\\t\\nInference\\t\\t\\n156\\n\\t\\nPositive Use Cases\\t\\n157\\nPreview (cont’d)\\nACCESS THE PUBLIC DATA\\n75\\nArtificial Intelligence\\nIndex Report 2024\\nCHAPTER 2:\\nTechnical \\nPerformance\\nTable of Contents',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   'Appendix\\n468\\nTable of Contents\\nArtificial Intelligence\\nIndex Report 2024\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2: Technical Performance\\nAppendix\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. & \\nSchulman, J. (2021). Training Verifiers to Solve Math Word Problems (arXiv:2110.14168). arXiv. http://arxiv.org/abs/2110.14168.\\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y. & Défossez, A. (2024). Simple and Controllable Music \\nGeneration (arXiv:2306.05284). arXiv. https://doi.org/10.48550/arXiv.2306.05284.\\nDettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs \\n(arXiv:2305.14314). arXiv. http://arxiv.org/abs/2305.14314.\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, \\nW., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., … Florence, P. \\n(2023). PaLM-E: An Embodied Multimodal Language Model (arXiv:2303.03378). arXiv. http://arxiv.org/abs/2303.03378.\\nGandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. (2023). Understanding Social Reasoning in Language Models \\nWith Language Models (arXiv:2306.15448). arXiv. http://arxiv.org/abs/2306.15448.\\nGemini Team: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., \\nSilver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). Gemini:  \\nA Family of Highly Capable Multimodal Models (arXiv:2312.11805). arXiv. http://arxiv.org/abs/2312.11805.\\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh, D. & Misra, I. (2023). Emu Video:',\n",
       "   '151\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2 Preview\\nTable of Contents\\n2.12 Techniques for LLM Improvement\\nChapter 2: Technical Performance\\nArtificial Intelligence\\nIndex Report 2024\\nFine-Tuning\\nFine-tuning has grown increasingly popular as a \\nmethod of enhancing LLMs and involves further \\ntraining or adjusting models on smaller datasets.  \\nFine-tuning not only boosts overall model \\nperformance but also sharpens the model’s \\ncapabilities on specific tasks. It also allows for more \\nprecise control over the model’s behavior.\\n879\\n902\\n916\\n966\\n974\\n992\\n1,022\\n1,348\\nGuanaco 7B\\nBard\\nGuanaco 13B\\nChatGPT\\nVicuna 13B\\nGuanaco 33B\\nGuanaco 65B\\nGPT-4\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\nElo rating (mean)\\nModel competitions based on 10,000 simulations using GPT-4 and the Vicuna benchmark\\nSource: Dettmers et al., 2023 | Chart: 2024 AI Index report\\nFigure 2.12.5\\nQLoRA\\nHighlighted Research:\\nQLoRA, developed by researchers from the \\nUniversity of Washington in 2023, is a new method \\nfor more efficient model fine-tuning. It dramatically \\nreduces memory usage, enabling the fine-tuning \\nof a 65 billion parameter model on a single 48 \\nGB GPU while maintaining full 16-bit fine-tuning \\nperformance. To put this in perspective, fine-tuning \\na 65B Llama model, a leading open-source LLM, \\ntypically requires about 780 GB of GPU memory. \\nTherefore, QLoRA is nearly 16 times more efficient.\\nQLoRA manages to increase efficiency with \\ntechniques like a 4-bit NormalFloat (NF4), double \\nquantization, and page optimizers. QLoRA is \\nused to train a model named Guanaco, which \\nmatched or even surpassed models like ChatGPT \\nin performance on the Vicuna benchmark (a \\nbenchmark that ranks the outputs of LLMs) (Figure \\n2.12.5). Remarkably, the Guanaco models were \\ncreated with just 24 hours of fine-tuning on a single \\nGPU. QLoRa highlights how methods for optimizing \\nand further improving models have become more \\nefficient, meaning fewer resources will be required \\nto make increasingly capable models.',\n",
       "   '151\\nArtificial Intelligence\\nIndex Report 2024\\nChapter 2 Preview\\nTable of Contents\\n2.12 Techniques for LLM Improvement\\nChapter 2: Technical Performance\\nArtificial Intelligence\\nIndex Report 2024\\nFine-Tuning\\nFine-tuning has grown increasingly popular as a \\nmethod of enhancing LLMs and involves further \\ntraining or adjusting models on smaller datasets.  \\nFine-tuning not only boosts overall model \\nperformance but also sharpens the model’s \\ncapabilities on specific tasks. It also allows for more \\nprecise control over the model’s behavior.\\n879\\n902\\n916\\n966\\n974\\n992\\n1,022\\n1,348\\nGuanaco 7B\\nBard\\nGuanaco 13B\\nChatGPT\\nVicuna 13B\\nGuanaco 33B\\nGuanaco 65B\\nGPT-4\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\nElo rating (mean)\\nModel competitions based on 10,000 simulations using GPT-4 and the Vicuna benchmark\\nSource: Dettmers et al., 2023 | Chart: 2024 AI Index report\\nFigure 2.12.5\\nQLoRA\\nHighlighted Research:\\nQLoRA, developed by researchers from the \\nUniversity of Washington in 2023, is a new method \\nfor more efficient model fine-tuning. It dramatically \\nreduces memory usage, enabling the fine-tuning \\nof a 65 billion parameter model on a single 48 \\nGB GPU while maintaining full 16-bit fine-tuning \\nperformance. To put this in perspective, fine-tuning \\na 65B Llama model, a leading open-source LLM, \\ntypically requires about 780 GB of GPU memory. \\nTherefore, QLoRA is nearly 16 times more efficient.\\nQLoRA manages to increase efficiency with \\ntechniques like a 4-bit NormalFloat (NF4), double \\nquantization, and page optimizers. QLoRA is \\nused to train a model named Guanaco, which \\nmatched or even surpassed models like ChatGPT \\nin performance on the Vicuna benchmark (a \\nbenchmark that ranks the outputs of LLMs) (Figure \\n2.12.5). Remarkably, the Guanaco models were \\ncreated with just 24 hours of fine-tuning on a single \\nGPU. QLoRa highlights how methods for optimizing \\nand further improving models have become more \\nefficient, meaning fewer resources will be required \\nto make increasingly capable models.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['distances', 'documents', 'metadatas']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanford_report_collection.query(\n",
    "    query_texts=[\"QLoRA\"],\n",
    "    where_document={\"$contains\":\"QLoRA\"},\n",
    "    n_results=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thisEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
